{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827e231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    \"json\", \n",
    "    data_files={\n",
    "        \"train\": \"archive/train.json\", \n",
    "        \"validation\": \"archive/dev.json\", \n",
    "        \"test\": \"archive/test.json\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca2faf",
   "metadata": {},
   "source": [
    "On fait marcher le modèle de Greg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb269956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"graph\")\n",
    "\n",
    "from graph.predict import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e47439bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local data: C:\\Users\\octav\\OneDrive\\Bureau\\hackathon\\HackEurope-2026-Hoga\\archive\n",
      "Loading dataset splits...\n",
      "  Loading train.json...\n",
      "  Loading dev.json...\n",
      "  Loading test.json...\n",
      "  Labeled users: 11826 (train=8278, val=2365, test=1183)\n",
      "  Total nodes: 191582 (11826 labeled, 179756 unlabeled neighbors)\n",
      "Building edge index...\n",
      "  Edges: 208716 (following=105701, follower=103015)\n",
      "Extracting node features...\n",
      "  Feature matrix: torch.Size([191582, 20])\n",
      "  Labels: 5237 human, 6589 bot, 179756 unlabeled\n",
      "  Masks: train=8278, val=2365, test=1183\n"
     ]
    }
   ],
   "source": [
    "probas = predict(list(raw_datasets[\"test\"]), model_type=\"rgcn\")\n",
    "prob1numpy = probas[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94eddd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = pd.DataFrame(prob1numpy, columns=[\"prob1\"])\n",
    "prob1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93e6df",
   "metadata": {},
   "source": [
    "On fait marcher le modèle d'Hadrien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789ee121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# # Hugging Face va automatiquement détecter et charger le fichier .safetensors\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"mon_modele_safetensors\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"mon_modele_safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d165c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'label', 'text'],\n",
      "        num_rows: 40426\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['ID', 'label', 'text'],\n",
      "        num_rows: 11546\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ID', 'label', 'text'],\n",
      "        num_rows: 5719\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "def explode_tweets(examples):\n",
    "    new_examples = {\"ID\": [], \"text\": [], \"label\": []}\n",
    "    \n",
    "    # On limite à 5 tweets par personne pour que ça aille très vite\n",
    "    MAX_TWEETS_PER_USER = 5\n",
    "    \n",
    "    for user_id, tweets, label in zip(examples[\"ID\"], examples[\"tweet\"], examples[\"label\"]):\n",
    "        if not tweets: \n",
    "            continue\n",
    "            \n",
    "        if isinstance(tweets, str): \n",
    "            tweets = [tweets]\n",
    "            \n",
    "        tweets_to_keep = tweets[:MAX_TWEETS_PER_USER]\n",
    "            \n",
    "        for tweet in tweets_to_keep:\n",
    "            if tweet: \n",
    "                new_examples[\"ID\"].append(user_id)\n",
    "                new_examples[\"text\"].append(str(tweet))\n",
    "                new_examples[\"label\"].append(int(label) if label is not None else 0)\n",
    "                \n",
    "    return new_examples\n",
    "\n",
    "# --- C'EST ICI QUE SE FAIT LA RÉDUCTION ---\n",
    "\n",
    "# 1. On réduit la taille des jeux de données d'origine\n",
    "small_train = raw_datasets[\"train\"].select(range(4000))\n",
    "small_valid = raw_datasets[\"validation\"].select(range(500))\n",
    "small_test = raw_datasets[\"test\"].select(range(500)) # On réduit le test aussi pour que l'évaluation soit rapide\n",
    "\n",
    "# 2. On les regroupe dans un DatasetDict (pour que la cellule de tokenisation fonctionne normalement)\n",
    "small_raw_datasets = DatasetDict({\n",
    "    \"train\": small_train,\n",
    "    \"validation\": small_valid,\n",
    "    \"test\": small_test\n",
    "})\n",
    "\n",
    "# 3. On applique notre fonction uniquement sur ce petit dataset\n",
    "processed_datasets = raw_datasets.map(\n",
    "    explode_tweets, \n",
    "    batched=True, \n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(processed_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012a3385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (40426, 3), 'validation': (11546, 3), 'test': (5719, 3)}\n",
      "{'train': (8278, 6), 'validation': (2365, 6), 'test': (1183, 6)}\n"
     ]
    }
   ],
   "source": [
    "print(processed_datasets.shape)\n",
    "print(raw_datasets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fed571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels : [{'label': 'LABEL_0', 'score': 0.9386758208274841}, {'label': 'LABEL_1', 'score': 0.061324186623096466}]\n",
      "                    ID  label  prob_bot\n",
      "0  1188812492010487808      1  0.061324\n",
      "1  1188812492010487808      1  0.253065\n",
      "2  1188812492010487808      1  0.785901\n",
      "3  1188812492010487808      1  0.801452\n",
      "4  1188812492010487808      1  0.697267\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Pipeline\n",
    "clf = pipeline(\"text-classification\", model=loaded_model, tokenizer=loaded_tokenizer, device=0)\n",
    "\n",
    "# Prédictions sur tout le test\n",
    "test_texts = list(processed_datasets[\"test\"][\"text\"])\n",
    "predictions = clf(test_texts, batch_size=32, top_k=None)\n",
    "\n",
    "# Vérifier les labels\n",
    "print(\"Labels :\", predictions[0])\n",
    "\n",
    "# Extraire proba LABEL_1\n",
    "probs = [\n",
    "    next(p[\"score\"] for p in pred if p[\"label\"] == \"LABEL_1\")\n",
    "    for pred in predictions\n",
    "]\n",
    "\n",
    "# Résultat\n",
    "df_pred = pd.DataFrame({\n",
    "    \"ID\": processed_datasets[\"test\"][\"ID\"],\n",
    "    \"label\": processed_datasets[\"test\"][\"label\"],\n",
    "    \"prob_bot\": probs\n",
    "})\n",
    "\n",
    "print(df_pred.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b613d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Regrouper par ID utilisateur et faire le Vote Majoritaire\n",
    "user_predictions = df_pred.groupby(\"ID\").agg(\n",
    "    # Le mode() prend la valeur la plus fréquente. [0] prend la première en cas d'égalité\n",
    "    majority_pred=(\"prob_bot\", lambda x: x.mode()[0]), \n",
    "    true_label=(\"label\", \"first\") # Le vrai label est le même pour tous les tweets de l'ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c47a88",
   "metadata": {},
   "outputs": [],
   "source": "# Agréger les probas tweet-level en probas user-level (moyenne par utilisateur)\n# df_pred contient les colonnes ID, label, prob_bot au niveau tweet\n# LABEL_1 du modèle Hadrien = \"human\", donc on inverse (1 - prob) pour avoir prob(bot)\nuser_prob3 = df_pred.groupby(\"ID\").agg(\n    prob3=(\"prob_bot\", \"mean\"),\n    label=(\"label\", \"first\")\n).reset_index()\n\nuser_prob3[\"prob3\"] = 1 - user_prob3[\"prob3\"]\n\nprint(f\"Nombre d'utilisateurs après agrégation: {user_prob3.shape[0]}\")\nuser_prob3.head(5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66449f3b",
   "metadata": {},
   "outputs": [],
   "source": "# Vérification des shapes\nprint(f\"prob1 shape: {prob1.shape}\")  # (1183, 1) - user-level\nprint(f\"prob2 shape (sera calculé après): attendu (1183, 1)\")  # user-level\nprint(f\"user_prob3 shape: {user_prob3.shape}\")  # (1173, 3) - user-level, 10 users sans tweets"
  },
  {
   "cell_type": "markdown",
   "id": "02836c63",
   "metadata": {},
   "source": [
    "On fait marcher le modèle de Aziz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb607931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.8.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.8.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import aziz_code\n",
    "importlib.reload(aziz_code)\n",
    "from aziz_code import create_csv, create_features3, predict_bot_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0ec328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# ── Charger le modèle ─────────────────────────────────────────────────────────\n",
    "with open('bot_detector.pkl', 'rb') as f:\n",
    "    saved = pickle.load(f)\n",
    "model    = saved['model']\n",
    "features = saved['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3de4626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1183, 20)\n"
     ]
    }
   ],
   "source": [
    "df = create_csv(raw_datasets[\"test\"])\n",
    "print(df.shape)\n",
    "df = create_features3(df)\n",
    "\n",
    "results = predict_bot_probability(df, model,features)\n",
    "prob2 = results[[\"prob_bot\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a647d",
   "metadata": {},
   "outputs": [],
   "source": "# Construire df_prob en s'assurant de l'alignement par ID\n# On utilise les IDs du test set comme référence\ntest_ids = [example[\"ID\"] for example in raw_datasets[\"test\"]]\n\ndf_prob = pd.DataFrame({\n    \"ID\": test_ids,\n    \"prob1\": prob1[\"prob1\"].values,\n    \"prob2\": prob2[\"prob_bot\"].values,\n    \"label\": [int(example[\"label\"]) for example in raw_datasets[\"test\"]]\n})\n\n# Merger prob3 (agrégé par utilisateur) sur l'ID pour garantir l'alignement\ndf_prob = df_prob.merge(user_prob3[[\"ID\", \"prob3\"]], on=\"ID\", how=\"left\")\n\n# Les utilisateurs sans tweets ont prob3=NaN → on remplit avec 0.5 (pas d'info)\nprint(f\"Utilisateurs sans tweets (NaN dans prob3): {df_prob['prob3'].isna().sum()}\")\ndf_prob[\"prob3\"] = df_prob[\"prob3\"].fillna(0.5)\n\nprint(f\"df_prob shape: {df_prob.shape}\")\ndf_prob.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63533e",
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\n# Histogrammes\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].hist(df_prob[\"prob1\"], bins=30)\naxes[0].set_title(\"prob1 (graphe)\")\naxes[1].hist(df_prob[\"prob2\"], bins=30)\naxes[1].set_title(\"prob2 (Aziz)\")\naxes[2].hist(df_prob[\"prob3\"], bins=30)\naxes[2].set_title(\"prob3 (Hadrien)\")\nplt.tight_layout()\nplt.show()\n\n# Corrélation\nprint(df_prob[[\"prob1\", \"prob2\", \"prob3\"]].corr())"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a2869d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24d403",
   "metadata": {},
   "source": [
    "on fait les trucs qui contiendront les différentes méthodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b25d2",
   "metadata": {},
   "source": [
    "On a df_prob qui a prob1, prob2, prob3 et label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66629242",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Labels in y_true and y_pred should be of the same type. Got y_true=['0' '1'] and y_pred=[0 1]. Make sure that the predictions provided by the classifier coincides with the true labels.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:126\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     unique_values = \u001b[43m_union1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# We expect y_true and y_pred to be of the same data type.\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# If `y_true` was provided to the classifier as strings,\u001b[39;00m\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# `y_pred` given by the classifier will also be encoded with\u001b[39;00m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# strings. So we raise a meaningful error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:220\u001b[39m, in \u001b[36m_union1d\u001b[39m\u001b[34m(a, b, xp)\u001b[39m\n\u001b[32m    219\u001b[39m     a_unique, b_unique = cached_unique(a, b, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_unique\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_unique\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m a.ndim == b.ndim == \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py:1176\u001b[39m, in \u001b[36munion1d\u001b[39m\u001b[34m(ar1, ar2)\u001b[39m\n\u001b[32m   1148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1149\u001b[39m \u001b[33;03mFind the union of two arrays.\u001b[39;00m\n\u001b[32m   1150\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1174\u001b[39m \u001b[33;03marray([1, 2, 3, 4, 6])\u001b[39;00m\n\u001b[32m   1175\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mar2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py:291\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     ret = \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py:358\u001b[39m, in \u001b[36m_unique1d\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, equal_nan, inverse_shape, axis)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m     \u001b[43mar\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m     aux = ar\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'int' and 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m acc1 = \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprob1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m acc2 = accuracy_score(df_prob[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m], (df_prob[\u001b[33m\"\u001b[39m\u001b[33mprob2\u001b[39m\u001b[33m\"\u001b[39m] >= \u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m))\n\u001b[32m      3\u001b[39m acc3 = accuracy_score(df_prob[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m], (df_prob[\u001b[33m\"\u001b[39m\u001b[33mprob3\u001b[39m\u001b[33m\"\u001b[39m] >= \u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:227\u001b[39m, in \u001b[36maccuracy_score\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[32m    226\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type.startswith(\u001b[33m\"\u001b[39m\u001b[33mmultilabel\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:132\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m    126\u001b[39m     unique_values = _union1d(y_true, y_pred, xp)\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# We expect y_true and y_pred to be of the same data type.\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# If `y_true` was provided to the classifier as strings,\u001b[39;00m\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# `y_pred` given by the classifier will also be encoded with\u001b[39;00m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# strings. So we raise a meaningful error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    133\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLabels in y_true and y_pred should be of the same type. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot y_true=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxp.unique(y_true)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my_pred=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxp.unique(y_pred)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Make sure that the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    136\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpredictions provided by the classifier coincides with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe true labels.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unique_values.shape[\u001b[32m0\u001b[39m] > \u001b[32m2\u001b[39m:\n\u001b[32m    140\u001b[39m     y_type = \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Labels in y_true and y_pred should be of the same type. Got y_true=['0' '1'] and y_pred=[0 1]. Make sure that the predictions provided by the classifier coincides with the true labels."
     ]
    }
   ],
   "source": [
    "acc1 = accuracy_score(df_prob[\"label\"], (df_prob[\"prob1\"] >= 0.5).astype(int))\n",
    "acc2 = accuracy_score(df_prob[\"label\"], (df_prob[\"prob2\"] >= 0.5).astype(int))\n",
    "acc3 = accuracy_score(df_prob[\"label\"], (df_prob[\"prob3\"] >= 0.5).astype(int))\n",
    "\n",
    "print(f\"Acc modèle 1: {acc1:.4f}\")\n",
    "print(f\"Acc modèle 2: {acc2:.4f}\")\n",
    "print(f\"Acc modèle 3: {acc3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultprob = pd.DataFrame()\n",
    "df_pred = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moyenne normale des probas\n",
    "df_resultprob[\"avg\"] = df_prob[[\"prob1\", \"prob2\", \"prob3\"]].mean(axis=1)\n",
    "df_pred[\"avg\"] = (df_resultprob[\"avg\"] >= 0.5).astype(int)\n",
    "print(f\"Acc avg: {accuracy_score(df_prob['label'], df_pred['avg']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeeb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moyenne pondérée par les accuracy\n",
    "total = acc1 + acc2 + acc3\n",
    "df_resultprob[\"wavg\"] = (acc1 * df_prob[\"prob1\"] + acc2 * df_prob[\"prob2\"] + acc3 * df_prob[\"prob3\"]) / total\n",
    "df_pred[\"wavg\"] = (df_resultprob[\"wavg\"] >= 0.5).astype(int)\n",
    "print(f\"Acc wavg: {accuracy_score(df_prob['label'], df_pred['wavg']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "lam = 0.01\n",
    "l1_ratio = 0.5\n",
    "\n",
    "enet = ElasticNet(alpha=lam, l1_ratio=l1_ratio)\n",
    "enet.fit(df_prob[[\"prob1\", \"prob2\", \"prob3\"]], df_prob[\"label\"])\n",
    "df_resultprob[\"enet\"] = enet.predict(df_prob[[\"prob1\", \"prob2\", \"prob3\"]]).clip(0, 1)\n",
    "df_pred[\"enet\"] = (df_resultprob[\"enet\"] >= 0.5).astype(int)\n",
    "print(f\"Acc ElasticNet (lambda={lam}, l1_ratio={l1_ratio}): {accuracy_score(df_prob['label'], df_pred['enet']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Régression logistique\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "df_resultprob[\"logreg\"] = np.nan\n",
    "for train_idx, val_idx in kf.split(df_prob):\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(df_prob.loc[train_idx, [\"prob1\", \"prob2\", \"prob3\"]], df_prob.loc[train_idx, \"label\"])\n",
    "    df_resultprob.loc[val_idx, \"logreg\"] = logreg.predict_proba(df_prob.loc[val_idx, [\"prob1\", \"prob2\", \"prob3\"]])[:, 1]\n",
    "\n",
    "df_pred[\"logreg\"] = (df_resultprob[\"logreg\"] >= 0.5).astype(int)\n",
    "print(f\"Acc LogReg (KFold): {accuracy_score(df_prob['label'], df_pred['logreg']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88330ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "df_resultprob[\"lgbm\"] = np.nan\n",
    "for train_idx, val_idx in kf.split(df_prob):\n",
    "    lgb_model = lgb.LGBMClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, verbose=-1)\n",
    "    lgb_model.fit(df_prob.loc[train_idx, [\"prob1\", \"prob2\", \"prob3\"]], df_prob.loc[train_idx, \"label\"])\n",
    "    df_resultprob.loc[val_idx, \"lgbm\"] = lgb_model.predict_proba(df_prob.loc[val_idx, [\"prob1\", \"prob2\", \"prob3\"]])[:, 1]\n",
    "\n",
    "df_pred[\"lgbm\"] = (df_resultprob[\"lgbm\"] >= 0.5).astype(int)\n",
    "print(f\"Acc LightGBM (KFold): {accuracy_score(df_prob['label'], df_pred['lgbm']):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}